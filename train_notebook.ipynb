{
  "cells": [
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyYQaEdwuQ1_",
        "outputId": "b61029b4-ff4c-4ee6-f10e-d698b87884a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Multilingual-PR'...\n",
            "remote: Enumerating objects: 1274, done.\u001b[K\n",
            "remote: Counting objects: 100% (1274/1274), done.\u001b[K\n",
            "remote: Compressing objects: 100% (838/838), done.\u001b[K\n",
            "remote: Total 1274 (delta 732), reused 888 (delta 407), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1274/1274), 1.93 MiB | 4.26 MiB/s, done.\n",
            "Resolving deltas: 100% (732/732), done.\n",
            "/content/Multilingual-PR\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 33.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 45.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 582 kB 37.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 92 kB 650 kB/s \n",
            "\u001b[K     |████████████████████████████████| 224 kB 47.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 316 kB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 39.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 46.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 37.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 42.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 36.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 398 kB 47.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 197 kB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41 kB 534 kB/s \n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data libespeak1 libportaudio2 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,219 kB of archives.\n",
            "After this operation, 3,031 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB]\n",
            "Fetched 1,219 kB in 1s (1,218 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-6) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak (1.48.04+dfsg-5) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-6) ...\n",
            "Setting up libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up espeak (1.48.04+dfsg-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/ASR-project/Multilingual-PR.git\n",
        "%cd Multilingual-PR\n",
        "!pip install -q -r requirements.txt\n",
=======
      "execution_count": null,
      "metadata": {
        "id": "kyYQaEdwuQ1_"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ASR-project/Multilingual-PR.git\n",
        "%cd Multilingual-PR\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install pytorch_lightning==1.5.10\n",
>>>>>>> 322ac4476aabd9fd840858cd74ba34fc870767b3
        "!apt-get install espeak -y"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OwxeTlzwXAV",
        "outputId": "43e801c9-5170-4371-aec8-5394ba9756d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
=======
      "execution_count": null,
      "metadata": {
        "id": "6OwxeTlzwXAV"
      },
      "outputs": [],
>>>>>>> 322ac4476aabd9fd840858cd74ba34fc870767b3
      "source": [
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "source": [
        "!pip install pytorch_lightning==1.5.10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PkVJJRfZ5HzM",
        "outputId": "0dcf0386-bbf4-4648-8b67-e01d21378d7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning==1.5.10\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (4.63.0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (0.7.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (1.21.5)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (2022.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (4.1.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (2.8.0)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (1.10.0+cu111)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.10) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning==1.5.10) (3.0.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (1.44.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.10) (0.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (1.25.11)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.5.10) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.10) (1.7.2)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=88242d9fcf454c8c6aa6252cf64aa2dd2a0d35fa127ad8ee5194859fd805ddf0\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: setuptools, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pyDeprecate\n",
            "    Found existing installation: pyDeprecate 0.3.2\n",
            "    Uninstalling pyDeprecate-0.3.2:\n",
            "      Successfully uninstalled pyDeprecate-0.3.2\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 1.6.0\n",
            "    Uninstalling pytorch-lightning-1.6.0:\n",
            "      Successfully uninstalled pytorch-lightning-1.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n",
            "Successfully installed future-0.18.2 pyDeprecate-0.3.1 pytorch-lightning-1.5.10 setuptools-59.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLg-sGwEFSoL",
        "outputId": "4b284fac-9acf-469a-ebbb-aae4695f0c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phonemizer language : ru\n",
            "Global seed set to 1776\n",
            "Pretrained model: microsoft/wavlm-large\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masr-project\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Multilingual-PR/wandb/run-20220401_081127-2y57fudp\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mWavLM_ru_tf_freezed\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/asr-project/test-asr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/asr-project/test-asr/runs/2y57fudp\u001b[0m\n",
            "08:11:31 [\u001b[36;1mINFO\u001b[0m]\t\tBaseTrainer\tCreate vocabulary language : ru ...\n",
            "08:11:32 [\u001b[36;1mINFO\u001b[0m]\t\tcreate_vocabulary\tLength vocabulary : 55\n",
            "08:11:32 [\u001b[36;1mINFO\u001b[0m]\t\tBaseTrainer\tVocabulary file : /content/Multilingual-PR/assets/vocab_phoneme/vocab-phoneme-ru.json\n",
            "08:11:32 [\u001b[36;1mINFO\u001b[0m]\t\tBaseTrainer\tLoading Data module...\n",
            "08:11:32 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tLoading Dataset : common_voice, language : ru\n",
            "08:11:32 [\u001b[36;1mINFO\u001b[0m]\t\tBaseTrainer\tLoading Model module...\n",
            "08:11:32 [\u001b[36;1mINFO\u001b[0m]\t\tBaseModule\tOptimizer : AdamW, lr : 0.02\n",
            "08:11:32 [\u001b[36;1mINFO\u001b[0m]\t\tBaseModule\tFeatures extractor : WavLM\n",
            "Downloading: 100% 2.17k/2.17k [00:00<00:00, 960kB/s]\n",
            "Downloading: 100% 1.18G/1.18G [00:53<00:00, 23.4MB/s]\n",
            "Some weights of WavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "08:12:31 [\u001b[36;1mINFO\u001b[0m]\t\tBaseModule\tModel: WavLM\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wavlm/modeling_wavlm.py:1371: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
            "  FutureWarning,\n",
            "08:12:31 [\u001b[36;1mINFO\u001b[0m]\t\tBaseModule\tFeature extactor: freezed\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/wandb.py:342: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "  \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "08:12:31 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tLoading the dataset in  load_data: train\n",
            "Downloading builder script: 26.4kB [00:00, 17.6MB/s]       \n",
            "Downloading metadata: 174kB [00:00, 45.3MB/s]        \n",
            "Downloading and preparing dataset common_voice/ru (download: 3.40 GiB, generated: 4.88 GiB, post-processed: Unknown size, total: 8.29 GiB) to /content/Multilingual-PR/assets/common_voice/ru/6.1.0/d3d5467c15716a2699f2ea3710fdc8bed7c20ae8ed66c248185735a0695dcc3b...\n",
            "Downloading data: 100% 3.66G/3.66G [01:51<00:00, 32.8MB/s]\n",
            "Dataset common_voice downloaded and prepared to /content/Multilingual-PR/assets/common_voice/ru/6.1.0/d3d5467c15716a2699f2ea3710fdc8bed7c20ae8ed66c248185735a0695dcc3b. Subsequent calls will reuse this data.\n",
            "08:19:35 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tSaved to assets/datasets/train_common_voice-ru/train_common_voice-ru\n",
            "08:19:35 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tDone prepare_data train\n",
            "08:19:35 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tProcessing train dataset ...\n",
            "100% 15481/15481 [00:06<00:00, 2271.67ex/s]\n",
            "  0% 0/31 [00:00<?, ?ba/s]/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py:169: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tensor = as_tensor(value)\n",
            "100% 31/31 [03:51<00:00,  7.48s/ba]\n",
            "08:23:34 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tSaving train dataset ...\n",
            "08:23:34 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tSaved to assets/datasets/train_common_voice-ru/train_common_voice-ru_process\n",
            "08:23:35 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tLoading the dataset in  load_data: val\n",
            "Reusing dataset common_voice (/content/Multilingual-PR/assets/common_voice/ru/6.1.0/d3d5467c15716a2699f2ea3710fdc8bed7c20ae8ed66c248185735a0695dcc3b)\n",
            "08:23:35 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tSaved to assets/datasets/val_common_voice-ru/val_common_voice-ru\n",
            "08:23:35 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tDone prepare_data val\n",
            "08:23:35 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tProcessing val dataset ...\n",
            "100% 7963/7963 [00:03<00:00, 2488.75ex/s]\n",
            "100% 16/16 [01:58<00:00,  7.41s/ba]\n",
            "08:25:37 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tSaving val dataset ...\n",
            "08:25:37 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tSaved to assets/datasets/val_common_voice-ru/val_common_voice-ru_process\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:292: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
            "  f\"Base `Callback.{hook}` hook signature has changed in v1.5.\"\n",
            "08:25:37 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tFiltering train dataset ...\n",
            "08:25:37 [\u001b[36;1mINFO\u001b[0m]\t\tBaseDataModule\tLength train dataset before filter 15481\n",
            "100% 15481/15481 [12:17<00:00, 21.00ex/s]\n",
            "tcmalloc: large alloc 1252966400 bytes == 0x56452dd7e000 @  0x7f37cab59615 0x5644004d117c 0x5644005b147a 0x5644004d3f9d 0x5644004d3ef0 0x5644005479a3 0x5644004d57aa 0x564400547d30 0x5644004d57aa 0x5644005438f6 0x564400542a2e 0x5644004d588a 0x5644005438f6 0x5644004d57aa 0x5644005438f6 0x564400542a2e 0x5644004d588a 0x5644005438f6 0x564400542a2e 0x5644004d613c 0x5644004d6341 0x564400544ff1 0x564400542a2e 0x5644004d588a 0x5644005438f6 0x5644004d57aa 0x5644005438f6 0x564400542a2e 0x5644004d588a 0x5644005438f6 0x564400542a2e\n",
            "tcmalloc: large alloc 1566212096 bytes == 0x56457886a000 @  0x7f37cab59615 0x5644004d117c 0x5644005b147a 0x5644004d3f9d 0x5644004d3ef0 0x5644005479a3 0x5644004d57aa 0x564400547d30 0x5644004d57aa 0x5644005438f6 0x564400542a2e 0x5644004d588a 0x5644005438f6 0x5644004d57aa 0x5644005438f6 0x564400542a2e 0x5644004d588a 0x5644005438f6 0x564400542a2e 0x5644004d613c 0x5644004d6341 0x564400544ff1 0x564400542a2e 0x5644004d588a 0x5644005438f6 0x5644004d57aa 0x5644005438f6 0x564400542a2e 0x5644004d588a 0x5644005438f6 0x564400542a2e\n",
            " 50% 8/16 [03:50<04:06, 30.85s/ba]"
          ]
        }
      ],
      "source": [
        "!python main.py --gpu 1 --num_workers 2 --language ru --subset ru --network_name WavLM --train True --num_proc 1 --enable_progress_bar False --lr 2e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq8g3K0UNDpf"
      },
      "outputs": [],
      "source": [
        ""
=======
      "execution_count": null,
      "metadata": {
        "id": "qLg-sGwEFSoL"
      },
      "outputs": [],
      "source": [
        "!python main.py --gpu 1 --num_workers 2 --language ru --subset ru --network_name WavLM --train True --num_proc 1 --enable_progress_bar False --lr 2e-2"
>>>>>>> 322ac4476aabd9fd840858cd74ba34fc870767b3
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
<<<<<<< HEAD
      "name": "Copy of Multilingual_project.ipynb",
=======
      "name": "train_notebook.ipynb",
>>>>>>> 322ac4476aabd9fd840858cd74ba34fc870767b3
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}